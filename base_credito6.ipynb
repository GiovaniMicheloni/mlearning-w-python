{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkHMQLMXEt+CR+2T/utkU5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovaniMicheloni/mlearning-w-python/blob/main/base_credito6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base Credito com algoritmo MLPClassifier (multilayer perceptron)"
      ],
      "metadata": {
        "id": "MdBe12k8_MBF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vawq_izW6eZ5"
      },
      "outputs": [],
      "source": [
        "!pip -q install plotly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q  install yellowbrick"
      ],
      "metadata": {
        "id": "R12tS5hc6nk4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qPIhvPvp6tVZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub9a2qa37MLP",
        "outputId": "9f016f77-7f77-4f3e-ca18-eefe8e479ebf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basecredito = pd.read_csv('/content/credit_data.csv')"
      ],
      "metadata": {
        "id": "pVKBHBGL7rvc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basecredito.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "SQZep5z58Dwp",
        "outputId": "c7dcb7d3-9868-4cd0-fe28-8515a4d5fe32"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          clientid        income          age          loan      default\n",
              "count  2000.000000   2000.000000  1997.000000   2000.000000  2000.000000\n",
              "mean   1000.500000  45331.600018    40.807559   4444.369695     0.141500\n",
              "std     577.494589  14326.327119    13.624469   3045.410024     0.348624\n",
              "min       1.000000  20014.489470   -52.423280      1.377630     0.000000\n",
              "25%     500.750000  32796.459717    28.990415   1939.708847     0.000000\n",
              "50%    1000.500000  45789.117313    41.317159   3974.719419     0.000000\n",
              "75%    1500.250000  57791.281668    52.587040   6432.410625     0.000000\n",
              "max    2000.000000  69995.685578    63.971796  13766.051239     1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-02cf94bc-d8d0-45f8-a7bd-a33a6bf2941d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clientid</th>\n",
              "      <th>income</th>\n",
              "      <th>age</th>\n",
              "      <th>loan</th>\n",
              "      <th>default</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>1997.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1000.500000</td>\n",
              "      <td>45331.600018</td>\n",
              "      <td>40.807559</td>\n",
              "      <td>4444.369695</td>\n",
              "      <td>0.141500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>577.494589</td>\n",
              "      <td>14326.327119</td>\n",
              "      <td>13.624469</td>\n",
              "      <td>3045.410024</td>\n",
              "      <td>0.348624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>20014.489470</td>\n",
              "      <td>-52.423280</td>\n",
              "      <td>1.377630</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>500.750000</td>\n",
              "      <td>32796.459717</td>\n",
              "      <td>28.990415</td>\n",
              "      <td>1939.708847</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1000.500000</td>\n",
              "      <td>45789.117313</td>\n",
              "      <td>41.317159</td>\n",
              "      <td>3974.719419</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1500.250000</td>\n",
              "      <td>57791.281668</td>\n",
              "      <td>52.587040</td>\n",
              "      <td>6432.410625</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2000.000000</td>\n",
              "      <td>69995.685578</td>\n",
              "      <td>63.971796</td>\n",
              "      <td>13766.051239</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02cf94bc-d8d0-45f8-a7bd-a33a6bf2941d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-02cf94bc-d8d0-45f8-a7bd-a33a6bf2941d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-02cf94bc-d8d0-45f8-a7bd-a33a6bf2941d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-83d94979-8459-4fde-b2f2-52255f362321\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-83d94979-8459-4fde-b2f2-52255f362321')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-83d94979-8459-4fde-b2f2-52255f362321 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"basecredito\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"clientid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 720.5049024934813,\n        \"min\": 1.0,\n        \"max\": 2000.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          2000.0,\n          1000.5,\n          1500.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23006.270052327105,\n        \"min\": 2000.0,\n        \"max\": 69995.6855783239,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          45331.600017793244,\n          45789.11731252445,\n          2000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 697.4194419741473,\n        \"min\": -52.4232799196616,\n        \"max\": 1997.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          40.80755937840458,\n          41.3171591130085,\n          1997.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"loan\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4225.971731499403,\n        \"min\": 1.37762959325451,\n        \"max\": 13766.0512393337,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          4444.369694688258,\n          3974.7194188426347,\n          2000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"default\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 707.0316018525249,\n        \"min\": 0.0,\n        \"max\": 2000.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.1415,\n          1.0,\n          0.34862375028048936\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basecredito.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLRJKHP38JbI",
        "outputId": "16e58d7e-f1de-48a9-8402-023e9d31a1af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2000 entries, 0 to 1999\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   clientid  2000 non-null   int64  \n",
            " 1   income    2000 non-null   float64\n",
            " 2   age       1997 non-null   float64\n",
            " 3   loan      2000 non-null   float64\n",
            " 4   default   2000 non-null   int64  \n",
            "dtypes: float64(3), int64(2)\n",
            "memory usage: 78.3 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basecredito[basecredito['age']<0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "5pd1jpX48OlI",
        "outputId": "0d3f1b39-b7e0-436e-c240-7a76bd3f10b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    clientid        income        age         loan  default\n",
              "15        16  50501.726689 -28.218361  3977.287432        0\n",
              "21        22  32197.620701 -52.423280  4244.057136        0\n",
              "26        27  63287.038908 -36.496976  9595.286289        0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f66108df-d922-48ec-a59f-ca112dcfa060\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clientid</th>\n",
              "      <th>income</th>\n",
              "      <th>age</th>\n",
              "      <th>loan</th>\n",
              "      <th>default</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>50501.726689</td>\n",
              "      <td>-28.218361</td>\n",
              "      <td>3977.287432</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22</td>\n",
              "      <td>32197.620701</td>\n",
              "      <td>-52.423280</td>\n",
              "      <td>4244.057136</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>63287.038908</td>\n",
              "      <td>-36.496976</td>\n",
              "      <td>9595.286289</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f66108df-d922-48ec-a59f-ca112dcfa060')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f66108df-d922-48ec-a59f-ca112dcfa060 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f66108df-d922-48ec-a59f-ca112dcfa060');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-08231ce2-35c6-4b74-bd05-e2cf41f7926f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-08231ce2-35c6-4b74-bd05-e2cf41f7926f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-08231ce2-35c6-4b74-bd05-e2cf41f7926f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"basecredito[basecredito['age']<0]\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"clientid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 16,\n        \"max\": 27,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          16,\n          22,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15626.134246775633,\n        \"min\": 32197.6207010448,\n        \"max\": 63287.038907874405,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          50501.7266888171,\n          32197.6207010448,\n          63287.038907874405\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.302172635538561,\n        \"min\": -52.4232799196616,\n        \"max\": -28.218361321371003,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -28.218361321371003,\n          -52.4232799196616,\n          -36.4969755136408\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"loan\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3169.351423445414,\n        \"min\": 3977.28743247384,\n        \"max\": 9595.28628892989,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3977.28743247384,\n          4244.057136123401,\n          9595.28628892989\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"default\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basecredito[basecredito['age']>0].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "froxTAkL8vlL",
        "outputId": "25246da9-9050-4242-c908-1fad2c44a275"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "clientid     1003.431795\n",
              "income      45328.856915\n",
              "age            40.927700\n",
              "loan         4443.240892\n",
              "default         0.141926\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>clientid</th>\n",
              "      <td>1003.431795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>income</th>\n",
              "      <td>45328.856915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>40.927700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>loan</th>\n",
              "      <td>4443.240892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>default</th>\n",
              "      <td>0.141926</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basecredito.loc[basecredito['age']<0, 'age'] = 40.92"
      ],
      "metadata": {
        "id": "-RlUC0bE9Av6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basecredito[basecredito['age']<0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "geo15_0t9P4Q",
        "outputId": "e5d40fed-cc72-4ce4-87c7-aae167454bc8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [clientid, income, age, loan, default]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f05dd7e-2dd3-4a48-95c5-191e5490e86e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clientid</th>\n",
              "      <th>income</th>\n",
              "      <th>age</th>\n",
              "      <th>loan</th>\n",
              "      <th>default</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f05dd7e-2dd3-4a48-95c5-191e5490e86e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0f05dd7e-2dd3-4a48-95c5-191e5490e86e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0f05dd7e-2dd3-4a48-95c5-191e5490e86e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"basecredito[basecredito['age']<0]\",\n  \"rows\": 0,\n  \"fields\": [\n    {\n      \"column\": \"clientid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"loan\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"default\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basecredito.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "aGzZAlld9R-N",
        "outputId": "48aac326-3b2e-4341-844a-4bace5a4c357"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "clientid    0\n",
              "income      0\n",
              "age         3\n",
              "loan        0\n",
              "default     0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>clientid</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>income</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>loan</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>default</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basecredito.loc[pd.isnull(basecredito['age'])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "daO1rZw_9XnM",
        "outputId": "82da2294-50e5-4b14-8011-b78235c46f75"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    clientid        income  age         loan  default\n",
              "28        29  59417.805406  NaN  2082.625938        0\n",
              "30        31  48528.852796  NaN  6155.784670        0\n",
              "31        32  23526.302555  NaN  2862.010139        0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f2991308-70b7-411f-8064-3b2ce6c454e2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clientid</th>\n",
              "      <th>income</th>\n",
              "      <th>age</th>\n",
              "      <th>loan</th>\n",
              "      <th>default</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>59417.805406</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2082.625938</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>48528.852796</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6155.784670</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>23526.302555</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2862.010139</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2991308-70b7-411f-8064-3b2ce6c454e2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f2991308-70b7-411f-8064-3b2ce6c454e2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f2991308-70b7-411f-8064-3b2ce6c454e2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-944669f1-2e75-42f8-811f-bee0e08b7b85\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-944669f1-2e75-42f8-811f-bee0e08b7b85')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-944669f1-2e75-42f8-811f-bee0e08b7b85 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"basecredito\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"clientid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 29,\n        \"max\": 32,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          29,\n          31,\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18402.430928200996,\n        \"min\": 23526.3025551103,\n        \"max\": 59417.805406265,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          59417.805406265,\n          48528.8527957164,\n          23526.3025551103\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"loan\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2162.0596836405384,\n        \"min\": 2082.62593812344,\n        \"max\": 6155.78467025544,\n        \"num_unique_values\": 3,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"default\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basecredito['age']= basecredito['age'].fillna(basecredito['age'].mean())"
      ],
      "metadata": {
        "id": "7wa0mbzK9gzg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xcredito = basecredito.iloc[:,1:4].values\n",
        "xcredito"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg1GSgAn9zbQ",
        "outputId": "e306b819-2ada-4ca9-8caa-2080245f5e8e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.61559251e+04, 5.90170151e+01, 8.10653213e+03],\n",
              "       [3.44151540e+04, 4.81171531e+01, 6.56474502e+03],\n",
              "       [5.73171701e+04, 6.31080495e+01, 8.02095330e+03],\n",
              "       ...,\n",
              "       [4.43114493e+04, 2.80171669e+01, 5.52278669e+03],\n",
              "       [4.37560566e+04, 6.39717958e+01, 1.62272260e+03],\n",
              "       [6.94365796e+04, 5.61526170e+01, 7.37883360e+03]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ycredito = basecredito.iloc[:,4].values\n",
        "ycredito"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLlt1Skf-E_P",
        "outputId": "5593eedd-71bd-4ebf-d252-b655db97804f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "standardscaler = StandardScaler()\n",
        "xcredito = standardscaler.fit_transform(xcredito)\n",
        "xcredito"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2u6Gr_Y-Lpb",
        "outputId": "17738c65-aad4-4fb3-f3a1-45e31b633d5b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.45393393,  1.36538093,  1.20281942],\n",
              "       [-0.76217555,  0.5426602 ,  0.69642695],\n",
              "       [ 0.83682073,  1.67417189,  1.17471147],\n",
              "       ...,\n",
              "       [-0.07122592, -0.97448519,  0.35420081],\n",
              "       [-0.11000289,  1.73936739, -0.92675625],\n",
              "       [ 1.682986  ,  1.14917639,  0.96381038]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xtreino, xtest, ytreino, ytest = train_test_split(xcredito, ycredito, test_size = 0.15, random_state =0)"
      ],
      "metadata": {
        "id": "xphhlStA-iDx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtest.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2lmHjBEEIBm",
        "outputId": "217525c1-6e9c-49d9-e863-dc91b40af2e0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "dTNDKscW-62n"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "redeneural_credit = MLPClassifier(max_iter =1500, verbose=True, tol=0.0000100, solver ='adam', activation='relu',hidden_layer_sizes=(2,2))\n",
        "redeneural_credit.fit(xtreino, ytreino)"
      ],
      "metadata": {
        "id": "hzdz_1ty_Dn1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0d25d4a-90b3-496f-867a-eb173851dc7d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.09298967\n",
            "Iteration 2, loss = 1.08085675\n",
            "Iteration 3, loss = 1.06843142\n",
            "Iteration 4, loss = 1.05580927\n",
            "Iteration 5, loss = 1.04298188\n",
            "Iteration 6, loss = 1.02987713\n",
            "Iteration 7, loss = 1.01656363\n",
            "Iteration 8, loss = 1.00295320\n",
            "Iteration 9, loss = 0.98902972\n",
            "Iteration 10, loss = 0.97516892\n",
            "Iteration 11, loss = 0.96113713\n",
            "Iteration 12, loss = 0.94695349\n",
            "Iteration 13, loss = 0.93246471\n",
            "Iteration 14, loss = 0.91795528\n",
            "Iteration 15, loss = 0.90336035\n",
            "Iteration 16, loss = 0.88865168\n",
            "Iteration 17, loss = 0.87396770\n",
            "Iteration 18, loss = 0.85921273\n",
            "Iteration 19, loss = 0.84474676\n",
            "Iteration 20, loss = 0.83018591\n",
            "Iteration 21, loss = 0.81560724\n",
            "Iteration 22, loss = 0.80126241\n",
            "Iteration 23, loss = 0.78716465\n",
            "Iteration 24, loss = 0.77307005\n",
            "Iteration 25, loss = 0.75925874\n",
            "Iteration 26, loss = 0.74553644\n",
            "Iteration 27, loss = 0.73211415\n",
            "Iteration 28, loss = 0.71877751\n",
            "Iteration 29, loss = 0.70518026\n",
            "Iteration 30, loss = 0.69215405\n",
            "Iteration 31, loss = 0.67941829\n",
            "Iteration 32, loss = 0.66698428\n",
            "Iteration 33, loss = 0.65492392\n",
            "Iteration 34, loss = 0.64305164\n",
            "Iteration 35, loss = 0.63116420\n",
            "Iteration 36, loss = 0.61983840\n",
            "Iteration 37, loss = 0.60870907\n",
            "Iteration 38, loss = 0.59777471\n",
            "Iteration 39, loss = 0.58722566\n",
            "Iteration 40, loss = 0.57665443\n",
            "Iteration 41, loss = 0.56649340\n",
            "Iteration 42, loss = 0.55650537\n",
            "Iteration 43, loss = 0.54661211\n",
            "Iteration 44, loss = 0.53706432\n",
            "Iteration 45, loss = 0.52768333\n",
            "Iteration 46, loss = 0.51842637\n",
            "Iteration 47, loss = 0.50887166\n",
            "Iteration 48, loss = 0.49949848\n",
            "Iteration 49, loss = 0.48947212\n",
            "Iteration 50, loss = 0.47958957\n",
            "Iteration 51, loss = 0.46937463\n",
            "Iteration 52, loss = 0.45890601\n",
            "Iteration 53, loss = 0.44895228\n",
            "Iteration 54, loss = 0.43928479\n",
            "Iteration 55, loss = 0.43054289\n",
            "Iteration 56, loss = 0.42206641\n",
            "Iteration 57, loss = 0.41455339\n",
            "Iteration 58, loss = 0.40738843\n",
            "Iteration 59, loss = 0.40082413\n",
            "Iteration 60, loss = 0.39434428\n",
            "Iteration 61, loss = 0.38851439\n",
            "Iteration 62, loss = 0.38275533\n",
            "Iteration 63, loss = 0.37748581\n",
            "Iteration 64, loss = 0.37239901\n",
            "Iteration 65, loss = 0.36744625\n",
            "Iteration 66, loss = 0.36277402\n",
            "Iteration 67, loss = 0.35834843\n",
            "Iteration 68, loss = 0.35402195\n",
            "Iteration 69, loss = 0.34987787\n",
            "Iteration 70, loss = 0.34592733\n",
            "Iteration 71, loss = 0.34216297\n",
            "Iteration 72, loss = 0.33851807\n",
            "Iteration 73, loss = 0.33499472\n",
            "Iteration 74, loss = 0.33166603\n",
            "Iteration 75, loss = 0.32841110\n",
            "Iteration 76, loss = 0.32535661\n",
            "Iteration 77, loss = 0.32231271\n",
            "Iteration 78, loss = 0.31930838\n",
            "Iteration 79, loss = 0.31654318\n",
            "Iteration 80, loss = 0.31385572\n",
            "Iteration 81, loss = 0.31122002\n",
            "Iteration 82, loss = 0.30866784\n",
            "Iteration 83, loss = 0.30624747\n",
            "Iteration 84, loss = 0.30391351\n",
            "Iteration 85, loss = 0.30163744\n",
            "Iteration 86, loss = 0.29942429\n",
            "Iteration 87, loss = 0.29723016\n",
            "Iteration 88, loss = 0.29509854\n",
            "Iteration 89, loss = 0.29298320\n",
            "Iteration 90, loss = 0.29094872\n",
            "Iteration 91, loss = 0.28886013\n",
            "Iteration 92, loss = 0.28680020\n",
            "Iteration 93, loss = 0.28477593\n",
            "Iteration 94, loss = 0.28278309\n",
            "Iteration 95, loss = 0.28074483\n",
            "Iteration 96, loss = 0.27874266\n",
            "Iteration 97, loss = 0.27687360\n",
            "Iteration 98, loss = 0.27498135\n",
            "Iteration 99, loss = 0.27309902\n",
            "Iteration 100, loss = 0.27129129\n",
            "Iteration 101, loss = 0.26947899\n",
            "Iteration 102, loss = 0.26766933\n",
            "Iteration 103, loss = 0.26592198\n",
            "Iteration 104, loss = 0.26412972\n",
            "Iteration 105, loss = 0.26232527\n",
            "Iteration 106, loss = 0.26055038\n",
            "Iteration 107, loss = 0.25878093\n",
            "Iteration 108, loss = 0.25708354\n",
            "Iteration 109, loss = 0.25532219\n",
            "Iteration 110, loss = 0.25362191\n",
            "Iteration 111, loss = 0.25194426\n",
            "Iteration 112, loss = 0.25020796\n",
            "Iteration 113, loss = 0.24851990\n",
            "Iteration 114, loss = 0.24686030\n",
            "Iteration 115, loss = 0.24525035\n",
            "Iteration 116, loss = 0.24357295\n",
            "Iteration 117, loss = 0.24195787\n",
            "Iteration 118, loss = 0.24034179\n",
            "Iteration 119, loss = 0.23876881\n",
            "Iteration 120, loss = 0.23717627\n",
            "Iteration 121, loss = 0.23563816\n",
            "Iteration 122, loss = 0.23406714\n",
            "Iteration 123, loss = 0.23251491\n",
            "Iteration 124, loss = 0.23096294\n",
            "Iteration 125, loss = 0.22946745\n",
            "Iteration 126, loss = 0.22791836\n",
            "Iteration 127, loss = 0.22639030\n",
            "Iteration 128, loss = 0.22486803\n",
            "Iteration 129, loss = 0.22336724\n",
            "Iteration 130, loss = 0.22186168\n",
            "Iteration 131, loss = 0.22039629\n",
            "Iteration 132, loss = 0.21890915\n",
            "Iteration 133, loss = 0.21746808\n",
            "Iteration 134, loss = 0.21602131\n",
            "Iteration 135, loss = 0.21460500\n",
            "Iteration 136, loss = 0.21314562\n",
            "Iteration 137, loss = 0.21176571\n",
            "Iteration 138, loss = 0.21039375\n",
            "Iteration 139, loss = 0.20903327\n",
            "Iteration 140, loss = 0.20767095\n",
            "Iteration 141, loss = 0.20631297\n",
            "Iteration 142, loss = 0.20499108\n",
            "Iteration 143, loss = 0.20369231\n",
            "Iteration 144, loss = 0.20242297\n",
            "Iteration 145, loss = 0.20115844\n",
            "Iteration 146, loss = 0.19992861\n",
            "Iteration 147, loss = 0.19876358\n",
            "Iteration 148, loss = 0.19758728\n",
            "Iteration 149, loss = 0.19636850\n",
            "Iteration 150, loss = 0.19523096\n",
            "Iteration 151, loss = 0.19405579\n",
            "Iteration 152, loss = 0.19290865\n",
            "Iteration 153, loss = 0.19177699\n",
            "Iteration 154, loss = 0.19071133\n",
            "Iteration 155, loss = 0.18955785\n",
            "Iteration 156, loss = 0.18848840\n",
            "Iteration 157, loss = 0.18739862\n",
            "Iteration 158, loss = 0.18637704\n",
            "Iteration 159, loss = 0.18534883\n",
            "Iteration 160, loss = 0.18431009\n",
            "Iteration 161, loss = 0.18331062\n",
            "Iteration 162, loss = 0.18227862\n",
            "Iteration 163, loss = 0.18127315\n",
            "Iteration 164, loss = 0.18026673\n",
            "Iteration 165, loss = 0.17928305\n",
            "Iteration 166, loss = 0.17832086\n",
            "Iteration 167, loss = 0.17731356\n",
            "Iteration 168, loss = 0.17632724\n",
            "Iteration 169, loss = 0.17535429\n",
            "Iteration 170, loss = 0.17441329\n",
            "Iteration 171, loss = 0.17343696\n",
            "Iteration 172, loss = 0.17248395\n",
            "Iteration 173, loss = 0.17151698\n",
            "Iteration 174, loss = 0.17054947\n",
            "Iteration 175, loss = 0.16963052\n",
            "Iteration 176, loss = 0.16871492\n",
            "Iteration 177, loss = 0.16780028\n",
            "Iteration 178, loss = 0.16690556\n",
            "Iteration 179, loss = 0.16599971\n",
            "Iteration 180, loss = 0.16511877\n",
            "Iteration 181, loss = 0.16424202\n",
            "Iteration 182, loss = 0.16336598\n",
            "Iteration 183, loss = 0.16247812\n",
            "Iteration 184, loss = 0.16163480\n",
            "Iteration 185, loss = 0.16078249\n",
            "Iteration 186, loss = 0.15992375\n",
            "Iteration 187, loss = 0.15911202\n",
            "Iteration 188, loss = 0.15826090\n",
            "Iteration 189, loss = 0.15742154\n",
            "Iteration 190, loss = 0.15659999\n",
            "Iteration 191, loss = 0.15582010\n",
            "Iteration 192, loss = 0.15498349\n",
            "Iteration 193, loss = 0.15418729\n",
            "Iteration 194, loss = 0.15338237\n",
            "Iteration 195, loss = 0.15257127\n",
            "Iteration 196, loss = 0.15178703\n",
            "Iteration 197, loss = 0.15098896\n",
            "Iteration 198, loss = 0.15018402\n",
            "Iteration 199, loss = 0.14938781\n",
            "Iteration 200, loss = 0.14863108\n",
            "Iteration 201, loss = 0.14783784\n",
            "Iteration 202, loss = 0.14704777\n",
            "Iteration 203, loss = 0.14628435\n",
            "Iteration 204, loss = 0.14553643\n",
            "Iteration 205, loss = 0.14475079\n",
            "Iteration 206, loss = 0.14399896\n",
            "Iteration 207, loss = 0.14322467\n",
            "Iteration 208, loss = 0.14248389\n",
            "Iteration 209, loss = 0.14175158\n",
            "Iteration 210, loss = 0.14100914\n",
            "Iteration 211, loss = 0.14027577\n",
            "Iteration 212, loss = 0.13951238\n",
            "Iteration 213, loss = 0.13876073\n",
            "Iteration 214, loss = 0.13802693\n",
            "Iteration 215, loss = 0.13729825\n",
            "Iteration 216, loss = 0.13657211\n",
            "Iteration 217, loss = 0.13585844\n",
            "Iteration 218, loss = 0.13511829\n",
            "Iteration 219, loss = 0.13442921\n",
            "Iteration 220, loss = 0.13371453\n",
            "Iteration 221, loss = 0.13296930\n",
            "Iteration 222, loss = 0.13227766\n",
            "Iteration 223, loss = 0.13155665\n",
            "Iteration 224, loss = 0.13085149\n",
            "Iteration 225, loss = 0.13016538\n",
            "Iteration 226, loss = 0.12946596\n",
            "Iteration 227, loss = 0.12878156\n",
            "Iteration 228, loss = 0.12810587\n",
            "Iteration 229, loss = 0.12740040\n",
            "Iteration 230, loss = 0.12671576\n",
            "Iteration 231, loss = 0.12607847\n",
            "Iteration 232, loss = 0.12533054\n",
            "Iteration 233, loss = 0.12466937\n",
            "Iteration 234, loss = 0.12402109\n",
            "Iteration 235, loss = 0.12332789\n",
            "Iteration 236, loss = 0.12265882\n",
            "Iteration 237, loss = 0.12199897\n",
            "Iteration 238, loss = 0.12134729\n",
            "Iteration 239, loss = 0.12067202\n",
            "Iteration 240, loss = 0.12003406\n",
            "Iteration 241, loss = 0.11941424\n",
            "Iteration 242, loss = 0.11877199\n",
            "Iteration 243, loss = 0.11812372\n",
            "Iteration 244, loss = 0.11747717\n",
            "Iteration 245, loss = 0.11684665\n",
            "Iteration 246, loss = 0.11621248\n",
            "Iteration 247, loss = 0.11558090\n",
            "Iteration 248, loss = 0.11497539\n",
            "Iteration 249, loss = 0.11433769\n",
            "Iteration 250, loss = 0.11373666\n",
            "Iteration 251, loss = 0.11311645\n",
            "Iteration 252, loss = 0.11250709\n",
            "Iteration 253, loss = 0.11188778\n",
            "Iteration 254, loss = 0.11127664\n",
            "Iteration 255, loss = 0.11068104\n",
            "Iteration 256, loss = 0.11008018\n",
            "Iteration 257, loss = 0.10944845\n",
            "Iteration 258, loss = 0.10885075\n",
            "Iteration 259, loss = 0.10824984\n",
            "Iteration 260, loss = 0.10765654\n",
            "Iteration 261, loss = 0.10707520\n",
            "Iteration 262, loss = 0.10649220\n",
            "Iteration 263, loss = 0.10589703\n",
            "Iteration 264, loss = 0.10532093\n",
            "Iteration 265, loss = 0.10474297\n",
            "Iteration 266, loss = 0.10416859\n",
            "Iteration 267, loss = 0.10359145\n",
            "Iteration 268, loss = 0.10302391\n",
            "Iteration 269, loss = 0.10255002\n",
            "Iteration 270, loss = 0.10221569\n",
            "Iteration 271, loss = 0.10186114\n",
            "Iteration 272, loss = 0.10151307\n",
            "Iteration 273, loss = 0.10116454\n",
            "Iteration 274, loss = 0.10084689\n",
            "Iteration 275, loss = 0.10054827\n",
            "Iteration 276, loss = 0.10022430\n",
            "Iteration 277, loss = 0.09988817\n",
            "Iteration 278, loss = 0.09956412\n",
            "Iteration 279, loss = 0.09926116\n",
            "Iteration 280, loss = 0.09894326\n",
            "Iteration 281, loss = 0.09862261\n",
            "Iteration 282, loss = 0.09832919\n",
            "Iteration 283, loss = 0.09801669\n",
            "Iteration 284, loss = 0.09769907\n",
            "Iteration 285, loss = 0.09740821\n",
            "Iteration 286, loss = 0.09710402\n",
            "Iteration 287, loss = 0.09680148\n",
            "Iteration 288, loss = 0.09653761\n",
            "Iteration 289, loss = 0.09622530\n",
            "Iteration 290, loss = 0.09593539\n",
            "Iteration 291, loss = 0.09563337\n",
            "Iteration 292, loss = 0.09537770\n",
            "Iteration 293, loss = 0.09506772\n",
            "Iteration 294, loss = 0.09482235\n",
            "Iteration 295, loss = 0.09452639\n",
            "Iteration 296, loss = 0.09421975\n",
            "Iteration 297, loss = 0.09398387\n",
            "Iteration 298, loss = 0.09367980\n",
            "Iteration 299, loss = 0.09341395\n",
            "Iteration 300, loss = 0.09313428\n",
            "Iteration 301, loss = 0.09286585\n",
            "Iteration 302, loss = 0.09259706\n",
            "Iteration 303, loss = 0.09233576\n",
            "Iteration 304, loss = 0.09206486\n",
            "Iteration 305, loss = 0.09179901\n",
            "Iteration 306, loss = 0.09155132\n",
            "Iteration 307, loss = 0.09127120\n",
            "Iteration 308, loss = 0.09099639\n",
            "Iteration 309, loss = 0.09073471\n",
            "Iteration 310, loss = 0.09046849\n",
            "Iteration 311, loss = 0.09020824\n",
            "Iteration 312, loss = 0.08994012\n",
            "Iteration 313, loss = 0.08967630\n",
            "Iteration 314, loss = 0.08941854\n",
            "Iteration 315, loss = 0.08916495\n",
            "Iteration 316, loss = 0.08890816\n",
            "Iteration 317, loss = 0.08861995\n",
            "Iteration 318, loss = 0.08833144\n",
            "Iteration 319, loss = 0.08808036\n",
            "Iteration 320, loss = 0.08781449\n",
            "Iteration 321, loss = 0.08754516\n",
            "Iteration 322, loss = 0.08729152\n",
            "Iteration 323, loss = 0.08703033\n",
            "Iteration 324, loss = 0.08677368\n",
            "Iteration 325, loss = 0.08652210\n",
            "Iteration 326, loss = 0.08626334\n",
            "Iteration 327, loss = 0.08605386\n",
            "Iteration 328, loss = 0.08574793\n",
            "Iteration 329, loss = 0.08550719\n",
            "Iteration 330, loss = 0.08525335\n",
            "Iteration 331, loss = 0.08497775\n",
            "Iteration 332, loss = 0.08473063\n",
            "Iteration 333, loss = 0.08448478\n",
            "Iteration 334, loss = 0.08425453\n",
            "Iteration 335, loss = 0.08401978\n",
            "Iteration 336, loss = 0.08373925\n",
            "Iteration 337, loss = 0.08350660\n",
            "Iteration 338, loss = 0.08326037\n",
            "Iteration 339, loss = 0.08300275\n",
            "Iteration 340, loss = 0.08280233\n",
            "Iteration 341, loss = 0.08252590\n",
            "Iteration 342, loss = 0.08229570\n",
            "Iteration 343, loss = 0.08204390\n",
            "Iteration 344, loss = 0.08180697\n",
            "Iteration 345, loss = 0.08158032\n",
            "Iteration 346, loss = 0.08135396\n",
            "Iteration 347, loss = 0.08112186\n",
            "Iteration 348, loss = 0.08087783\n",
            "Iteration 349, loss = 0.08062164\n",
            "Iteration 350, loss = 0.08039603\n",
            "Iteration 351, loss = 0.08018500\n",
            "Iteration 352, loss = 0.07993407\n",
            "Iteration 353, loss = 0.07969582\n",
            "Iteration 354, loss = 0.07945801\n",
            "Iteration 355, loss = 0.07923442\n",
            "Iteration 356, loss = 0.07901983\n",
            "Iteration 357, loss = 0.07879493\n",
            "Iteration 358, loss = 0.07854544\n",
            "Iteration 359, loss = 0.07831282\n",
            "Iteration 360, loss = 0.07808011\n",
            "Iteration 361, loss = 0.07782961\n",
            "Iteration 362, loss = 0.07760942\n",
            "Iteration 363, loss = 0.07736494\n",
            "Iteration 364, loss = 0.07712115\n",
            "Iteration 365, loss = 0.07688099\n",
            "Iteration 366, loss = 0.07665577\n",
            "Iteration 367, loss = 0.07641573\n",
            "Iteration 368, loss = 0.07617621\n",
            "Iteration 369, loss = 0.07594959\n",
            "Iteration 370, loss = 0.07571610\n",
            "Iteration 371, loss = 0.07549383\n",
            "Iteration 372, loss = 0.07527796\n",
            "Iteration 373, loss = 0.07503581\n",
            "Iteration 374, loss = 0.07482018\n",
            "Iteration 375, loss = 0.07458432\n",
            "Iteration 376, loss = 0.07435702\n",
            "Iteration 377, loss = 0.07412127\n",
            "Iteration 378, loss = 0.07390418\n",
            "Iteration 379, loss = 0.07368385\n",
            "Iteration 380, loss = 0.07345418\n",
            "Iteration 381, loss = 0.07321621\n",
            "Iteration 382, loss = 0.07301323\n",
            "Iteration 383, loss = 0.07279710\n",
            "Iteration 384, loss = 0.07257648\n",
            "Iteration 385, loss = 0.07234518\n",
            "Iteration 386, loss = 0.07211928\n",
            "Iteration 387, loss = 0.07192528\n",
            "Iteration 388, loss = 0.07168398\n",
            "Iteration 389, loss = 0.07146925\n",
            "Iteration 390, loss = 0.07126181\n",
            "Iteration 391, loss = 0.07103056\n",
            "Iteration 392, loss = 0.07080795\n",
            "Iteration 393, loss = 0.07057823\n",
            "Iteration 394, loss = 0.07037530\n",
            "Iteration 395, loss = 0.07014821\n",
            "Iteration 396, loss = 0.06995448\n",
            "Iteration 397, loss = 0.06972357\n",
            "Iteration 398, loss = 0.06951064\n",
            "Iteration 399, loss = 0.06930117\n",
            "Iteration 400, loss = 0.06907838\n",
            "Iteration 401, loss = 0.06888239\n",
            "Iteration 402, loss = 0.06867804\n",
            "Iteration 403, loss = 0.06845824\n",
            "Iteration 404, loss = 0.06821909\n",
            "Iteration 405, loss = 0.06804365\n",
            "Iteration 406, loss = 0.06779136\n",
            "Iteration 407, loss = 0.06756813\n",
            "Iteration 408, loss = 0.06736438\n",
            "Iteration 409, loss = 0.06717028\n",
            "Iteration 410, loss = 0.06697637\n",
            "Iteration 411, loss = 0.06672163\n",
            "Iteration 412, loss = 0.06653945\n",
            "Iteration 413, loss = 0.06630945\n",
            "Iteration 414, loss = 0.06610175\n",
            "Iteration 415, loss = 0.06588900\n",
            "Iteration 416, loss = 0.06567899\n",
            "Iteration 417, loss = 0.06548986\n",
            "Iteration 418, loss = 0.06527452\n",
            "Iteration 419, loss = 0.06505960\n",
            "Iteration 420, loss = 0.06486456\n",
            "Iteration 421, loss = 0.06467348\n",
            "Iteration 422, loss = 0.06445730\n",
            "Iteration 423, loss = 0.06424004\n",
            "Iteration 424, loss = 0.06403512\n",
            "Iteration 425, loss = 0.06383818\n",
            "Iteration 426, loss = 0.06363811\n",
            "Iteration 427, loss = 0.06345574\n",
            "Iteration 428, loss = 0.06323537\n",
            "Iteration 429, loss = 0.06303760\n",
            "Iteration 430, loss = 0.06285261\n",
            "Iteration 431, loss = 0.06264101\n",
            "Iteration 432, loss = 0.06245923\n",
            "Iteration 433, loss = 0.06223837\n",
            "Iteration 434, loss = 0.06205308\n",
            "Iteration 435, loss = 0.06184794\n",
            "Iteration 436, loss = 0.06166206\n",
            "Iteration 437, loss = 0.06147338\n",
            "Iteration 438, loss = 0.06127716\n",
            "Iteration 439, loss = 0.06109834\n",
            "Iteration 440, loss = 0.06087807\n",
            "Iteration 441, loss = 0.06070179\n",
            "Iteration 442, loss = 0.06052869\n",
            "Iteration 443, loss = 0.06032714\n",
            "Iteration 444, loss = 0.06014089\n",
            "Iteration 445, loss = 0.05995502\n",
            "Iteration 446, loss = 0.05976487\n",
            "Iteration 447, loss = 0.05958090\n",
            "Iteration 448, loss = 0.05939001\n",
            "Iteration 449, loss = 0.05920152\n",
            "Iteration 450, loss = 0.05903765\n",
            "Iteration 451, loss = 0.05886318\n",
            "Iteration 452, loss = 0.05869086\n",
            "Iteration 453, loss = 0.05850285\n",
            "Iteration 454, loss = 0.05831695\n",
            "Iteration 455, loss = 0.05817054\n",
            "Iteration 456, loss = 0.05795611\n",
            "Iteration 457, loss = 0.05777984\n",
            "Iteration 458, loss = 0.05760199\n",
            "Iteration 459, loss = 0.05743641\n",
            "Iteration 460, loss = 0.05725510\n",
            "Iteration 461, loss = 0.05708425\n",
            "Iteration 462, loss = 0.05690147\n",
            "Iteration 463, loss = 0.05674868\n",
            "Iteration 464, loss = 0.05655051\n",
            "Iteration 465, loss = 0.05638333\n",
            "Iteration 466, loss = 0.05622194\n",
            "Iteration 467, loss = 0.05604114\n",
            "Iteration 468, loss = 0.05589621\n",
            "Iteration 469, loss = 0.05572571\n",
            "Iteration 470, loss = 0.05556056\n",
            "Iteration 471, loss = 0.05537081\n",
            "Iteration 472, loss = 0.05522382\n",
            "Iteration 473, loss = 0.05506326\n",
            "Iteration 474, loss = 0.05487730\n",
            "Iteration 475, loss = 0.05471849\n",
            "Iteration 476, loss = 0.05455226\n",
            "Iteration 477, loss = 0.05441349\n",
            "Iteration 478, loss = 0.05425111\n",
            "Iteration 479, loss = 0.05406138\n",
            "Iteration 480, loss = 0.05390068\n",
            "Iteration 481, loss = 0.05373823\n",
            "Iteration 482, loss = 0.05358442\n",
            "Iteration 483, loss = 0.05340616\n",
            "Iteration 484, loss = 0.05328503\n",
            "Iteration 485, loss = 0.05309719\n",
            "Iteration 486, loss = 0.05293794\n",
            "Iteration 487, loss = 0.05279718\n",
            "Iteration 488, loss = 0.05261307\n",
            "Iteration 489, loss = 0.05247190\n",
            "Iteration 490, loss = 0.05229113\n",
            "Iteration 491, loss = 0.05213467\n",
            "Iteration 492, loss = 0.05197439\n",
            "Iteration 493, loss = 0.05184738\n",
            "Iteration 494, loss = 0.05168780\n",
            "Iteration 495, loss = 0.05153004\n",
            "Iteration 496, loss = 0.05134446\n",
            "Iteration 497, loss = 0.05120765\n",
            "Iteration 498, loss = 0.05105128\n",
            "Iteration 499, loss = 0.05087957\n",
            "Iteration 500, loss = 0.05073178\n",
            "Iteration 501, loss = 0.05057081\n",
            "Iteration 502, loss = 0.05041839\n",
            "Iteration 503, loss = 0.05025970\n",
            "Iteration 504, loss = 0.05010942\n",
            "Iteration 505, loss = 0.04995452\n",
            "Iteration 506, loss = 0.04979591\n",
            "Iteration 507, loss = 0.04962824\n",
            "Iteration 508, loss = 0.04951493\n",
            "Iteration 509, loss = 0.04932858\n",
            "Iteration 510, loss = 0.04918380\n",
            "Iteration 511, loss = 0.04904058\n",
            "Iteration 512, loss = 0.04886936\n",
            "Iteration 513, loss = 0.04872157\n",
            "Iteration 514, loss = 0.04857102\n",
            "Iteration 515, loss = 0.04842849\n",
            "Iteration 516, loss = 0.04828232\n",
            "Iteration 517, loss = 0.04811007\n",
            "Iteration 518, loss = 0.04796289\n",
            "Iteration 519, loss = 0.04785882\n",
            "Iteration 520, loss = 0.04767758\n",
            "Iteration 521, loss = 0.04753223\n",
            "Iteration 522, loss = 0.04736608\n",
            "Iteration 523, loss = 0.04722411\n",
            "Iteration 524, loss = 0.04707378\n",
            "Iteration 525, loss = 0.04692816\n",
            "Iteration 526, loss = 0.04679457\n",
            "Iteration 527, loss = 0.04665305\n",
            "Iteration 528, loss = 0.04649902\n",
            "Iteration 529, loss = 0.04637134\n",
            "Iteration 530, loss = 0.04621952\n",
            "Iteration 531, loss = 0.04604581\n",
            "Iteration 532, loss = 0.04591492\n",
            "Iteration 533, loss = 0.04575879\n",
            "Iteration 534, loss = 0.04561117\n",
            "Iteration 535, loss = 0.04547287\n",
            "Iteration 536, loss = 0.04533821\n",
            "Iteration 537, loss = 0.04518308\n",
            "Iteration 538, loss = 0.04505137\n",
            "Iteration 539, loss = 0.04490493\n",
            "Iteration 540, loss = 0.04475697\n",
            "Iteration 541, loss = 0.04462456\n",
            "Iteration 542, loss = 0.04448026\n",
            "Iteration 543, loss = 0.04436288\n",
            "Iteration 544, loss = 0.04419181\n",
            "Iteration 545, loss = 0.04404988\n",
            "Iteration 546, loss = 0.04391758\n",
            "Iteration 547, loss = 0.04376277\n",
            "Iteration 548, loss = 0.04363810\n",
            "Iteration 549, loss = 0.04349444\n",
            "Iteration 550, loss = 0.04337195\n",
            "Iteration 551, loss = 0.04322302\n",
            "Iteration 552, loss = 0.04310044\n",
            "Iteration 553, loss = 0.04295678\n",
            "Iteration 554, loss = 0.04280369\n",
            "Iteration 555, loss = 0.04269612\n",
            "Iteration 556, loss = 0.04253668\n",
            "Iteration 557, loss = 0.04239952\n",
            "Iteration 558, loss = 0.04229603\n",
            "Iteration 559, loss = 0.04210728\n",
            "Iteration 560, loss = 0.04200419\n",
            "Iteration 561, loss = 0.04186615\n",
            "Iteration 562, loss = 0.04171906\n",
            "Iteration 563, loss = 0.04161803\n",
            "Iteration 564, loss = 0.04147065\n",
            "Iteration 565, loss = 0.04134269\n",
            "Iteration 566, loss = 0.04122020\n",
            "Iteration 567, loss = 0.04110320\n",
            "Iteration 568, loss = 0.04093020\n",
            "Iteration 569, loss = 0.04079879\n",
            "Iteration 570, loss = 0.04067443\n",
            "Iteration 571, loss = 0.04053604\n",
            "Iteration 572, loss = 0.04042314\n",
            "Iteration 573, loss = 0.04029853\n",
            "Iteration 574, loss = 0.04016782\n",
            "Iteration 575, loss = 0.04004269\n",
            "Iteration 576, loss = 0.03991825\n",
            "Iteration 577, loss = 0.03981935\n",
            "Iteration 578, loss = 0.03966416\n",
            "Iteration 579, loss = 0.03952142\n",
            "Iteration 580, loss = 0.03941534\n",
            "Iteration 581, loss = 0.03928615\n",
            "Iteration 582, loss = 0.03915425\n",
            "Iteration 583, loss = 0.03902374\n",
            "Iteration 584, loss = 0.03889568\n",
            "Iteration 585, loss = 0.03877864\n",
            "Iteration 586, loss = 0.03866030\n",
            "Iteration 587, loss = 0.03855080\n",
            "Iteration 588, loss = 0.03841271\n",
            "Iteration 589, loss = 0.03830042\n",
            "Iteration 590, loss = 0.03817735\n",
            "Iteration 591, loss = 0.03805398\n",
            "Iteration 592, loss = 0.03791587\n",
            "Iteration 593, loss = 0.03779834\n",
            "Iteration 594, loss = 0.03767368\n",
            "Iteration 595, loss = 0.03755120\n",
            "Iteration 596, loss = 0.03744248\n",
            "Iteration 597, loss = 0.03731716\n",
            "Iteration 598, loss = 0.03720818\n",
            "Iteration 599, loss = 0.03709683\n",
            "Iteration 600, loss = 0.03697797\n",
            "Iteration 601, loss = 0.03684026\n",
            "Iteration 602, loss = 0.03673430\n",
            "Iteration 603, loss = 0.03661169\n",
            "Iteration 604, loss = 0.03652394\n",
            "Iteration 605, loss = 0.03636945\n",
            "Iteration 606, loss = 0.03625871\n",
            "Iteration 607, loss = 0.03616899\n",
            "Iteration 608, loss = 0.03600939\n",
            "Iteration 609, loss = 0.03592594\n",
            "Iteration 610, loss = 0.03580199\n",
            "Iteration 611, loss = 0.03567192\n",
            "Iteration 612, loss = 0.03555567\n",
            "Iteration 613, loss = 0.03543908\n",
            "Iteration 614, loss = 0.03533583\n",
            "Iteration 615, loss = 0.03523323\n",
            "Iteration 616, loss = 0.03511526\n",
            "Iteration 617, loss = 0.03497201\n",
            "Iteration 618, loss = 0.03491678\n",
            "Iteration 619, loss = 0.03475155\n",
            "Iteration 620, loss = 0.03464233\n",
            "Iteration 621, loss = 0.03452553\n",
            "Iteration 622, loss = 0.03444019\n",
            "Iteration 623, loss = 0.03431632\n",
            "Iteration 624, loss = 0.03420086\n",
            "Iteration 625, loss = 0.03413185\n",
            "Iteration 626, loss = 0.03400839\n",
            "Iteration 627, loss = 0.03386323\n",
            "Iteration 628, loss = 0.03378299\n",
            "Iteration 629, loss = 0.03365552\n",
            "Iteration 630, loss = 0.03355023\n",
            "Iteration 631, loss = 0.03346302\n",
            "Iteration 632, loss = 0.03334594\n",
            "Iteration 633, loss = 0.03323719\n",
            "Iteration 634, loss = 0.03310861\n",
            "Iteration 635, loss = 0.03302792\n",
            "Iteration 636, loss = 0.03291190\n",
            "Iteration 637, loss = 0.03280693\n",
            "Iteration 638, loss = 0.03272999\n",
            "Iteration 639, loss = 0.03258629\n",
            "Iteration 640, loss = 0.03248663\n",
            "Iteration 641, loss = 0.03237150\n",
            "Iteration 642, loss = 0.03226848\n",
            "Iteration 643, loss = 0.03220432\n",
            "Iteration 644, loss = 0.03207554\n",
            "Iteration 645, loss = 0.03197137\n",
            "Iteration 646, loss = 0.03184600\n",
            "Iteration 647, loss = 0.03174545\n",
            "Iteration 648, loss = 0.03164266\n",
            "Iteration 649, loss = 0.03154230\n",
            "Iteration 650, loss = 0.03143948\n",
            "Iteration 651, loss = 0.03133883\n",
            "Iteration 652, loss = 0.03125645\n",
            "Iteration 653, loss = 0.03114409\n",
            "Iteration 654, loss = 0.03107125\n",
            "Iteration 655, loss = 0.03095086\n",
            "Iteration 656, loss = 0.03085252\n",
            "Iteration 657, loss = 0.03076266\n",
            "Iteration 658, loss = 0.03064254\n",
            "Iteration 659, loss = 0.03057793\n",
            "Iteration 660, loss = 0.03046464\n",
            "Iteration 661, loss = 0.03036043\n",
            "Iteration 662, loss = 0.03027763\n",
            "Iteration 663, loss = 0.03017558\n",
            "Iteration 664, loss = 0.03010288\n",
            "Iteration 665, loss = 0.03001125\n",
            "Iteration 666, loss = 0.02991459\n",
            "Iteration 667, loss = 0.02978052\n",
            "Iteration 668, loss = 0.02969987\n",
            "Iteration 669, loss = 0.02959024\n",
            "Iteration 670, loss = 0.02951580\n",
            "Iteration 671, loss = 0.02942363\n",
            "Iteration 672, loss = 0.02940356\n",
            "Iteration 673, loss = 0.02926021\n",
            "Iteration 674, loss = 0.02914131\n",
            "Iteration 675, loss = 0.02904843\n",
            "Iteration 676, loss = 0.02896696\n",
            "Iteration 677, loss = 0.02889115\n",
            "Iteration 678, loss = 0.02876539\n",
            "Iteration 679, loss = 0.02870218\n",
            "Iteration 680, loss = 0.02860109\n",
            "Iteration 681, loss = 0.02852281\n",
            "Iteration 682, loss = 0.02843494\n",
            "Iteration 683, loss = 0.02833928\n",
            "Iteration 684, loss = 0.02824474\n",
            "Iteration 685, loss = 0.02818601\n",
            "Iteration 686, loss = 0.02804287\n",
            "Iteration 687, loss = 0.02797408\n",
            "Iteration 688, loss = 0.02791686\n",
            "Iteration 689, loss = 0.02780276\n",
            "Iteration 690, loss = 0.02772311\n",
            "Iteration 691, loss = 0.02764448\n",
            "Iteration 692, loss = 0.02757170\n",
            "Iteration 693, loss = 0.02746527\n",
            "Iteration 694, loss = 0.02734945\n",
            "Iteration 695, loss = 0.02732037\n",
            "Iteration 696, loss = 0.02723546\n",
            "Iteration 697, loss = 0.02714844\n",
            "Iteration 698, loss = 0.02703423\n",
            "Iteration 699, loss = 0.02699798\n",
            "Iteration 700, loss = 0.02686856\n",
            "Iteration 701, loss = 0.02675728\n",
            "Iteration 702, loss = 0.02669176\n",
            "Iteration 703, loss = 0.02660932\n",
            "Iteration 704, loss = 0.02651330\n",
            "Iteration 705, loss = 0.02647487\n",
            "Iteration 706, loss = 0.02634700\n",
            "Iteration 707, loss = 0.02627391\n",
            "Iteration 708, loss = 0.02618895\n",
            "Iteration 709, loss = 0.02610871\n",
            "Iteration 710, loss = 0.02601257\n",
            "Iteration 711, loss = 0.02594059\n",
            "Iteration 712, loss = 0.02592543\n",
            "Iteration 713, loss = 0.02577178\n",
            "Iteration 714, loss = 0.02570950\n",
            "Iteration 715, loss = 0.02562237\n",
            "Iteration 716, loss = 0.02552925\n",
            "Iteration 717, loss = 0.02546816\n",
            "Iteration 718, loss = 0.02537233\n",
            "Iteration 719, loss = 0.02528833\n",
            "Iteration 720, loss = 0.02521324\n",
            "Iteration 721, loss = 0.02519721\n",
            "Iteration 722, loss = 0.02508544\n",
            "Iteration 723, loss = 0.02498305\n",
            "Iteration 724, loss = 0.02490907\n",
            "Iteration 725, loss = 0.02482210\n",
            "Iteration 726, loss = 0.02474833\n",
            "Iteration 727, loss = 0.02465468\n",
            "Iteration 728, loss = 0.02459879\n",
            "Iteration 729, loss = 0.02450509\n",
            "Iteration 730, loss = 0.02443356\n",
            "Iteration 731, loss = 0.02433420\n",
            "Iteration 732, loss = 0.02425745\n",
            "Iteration 733, loss = 0.02418807\n",
            "Iteration 734, loss = 0.02411348\n",
            "Iteration 735, loss = 0.02404851\n",
            "Iteration 736, loss = 0.02395992\n",
            "Iteration 737, loss = 0.02389376\n",
            "Iteration 738, loss = 0.02379642\n",
            "Iteration 739, loss = 0.02373127\n",
            "Iteration 740, loss = 0.02368792\n",
            "Iteration 741, loss = 0.02356822\n",
            "Iteration 742, loss = 0.02349369\n",
            "Iteration 743, loss = 0.02342329\n",
            "Iteration 744, loss = 0.02338748\n",
            "Iteration 745, loss = 0.02330790\n",
            "Iteration 746, loss = 0.02321347\n",
            "Iteration 747, loss = 0.02314992\n",
            "Iteration 748, loss = 0.02306400\n",
            "Iteration 749, loss = 0.02298750\n",
            "Iteration 750, loss = 0.02290257\n",
            "Iteration 751, loss = 0.02282354\n",
            "Iteration 752, loss = 0.02275904\n",
            "Iteration 753, loss = 0.02269042\n",
            "Iteration 754, loss = 0.02261280\n",
            "Iteration 755, loss = 0.02251401\n",
            "Iteration 756, loss = 0.02246376\n",
            "Iteration 757, loss = 0.02238662\n",
            "Iteration 758, loss = 0.02231199\n",
            "Iteration 759, loss = 0.02227055\n",
            "Iteration 760, loss = 0.02215846\n",
            "Iteration 761, loss = 0.02209036\n",
            "Iteration 762, loss = 0.02201755\n",
            "Iteration 763, loss = 0.02195118\n",
            "Iteration 764, loss = 0.02188152\n",
            "Iteration 765, loss = 0.02182164\n",
            "Iteration 766, loss = 0.02175068\n",
            "Iteration 767, loss = 0.02165501\n",
            "Iteration 768, loss = 0.02160039\n",
            "Iteration 769, loss = 0.02154683\n",
            "Iteration 770, loss = 0.02145676\n",
            "Iteration 771, loss = 0.02138289\n",
            "Iteration 772, loss = 0.02131396\n",
            "Iteration 773, loss = 0.02126293\n",
            "Iteration 774, loss = 0.02117392\n",
            "Iteration 775, loss = 0.02114939\n",
            "Iteration 776, loss = 0.02103605\n",
            "Iteration 777, loss = 0.02096567\n",
            "Iteration 778, loss = 0.02095026\n",
            "Iteration 779, loss = 0.02090333\n",
            "Iteration 780, loss = 0.02080607\n",
            "Iteration 781, loss = 0.02071779\n",
            "Iteration 782, loss = 0.02064610\n",
            "Iteration 783, loss = 0.02058156\n",
            "Iteration 784, loss = 0.02051970\n",
            "Iteration 785, loss = 0.02043730\n",
            "Iteration 786, loss = 0.02043002\n",
            "Iteration 787, loss = 0.02037845\n",
            "Iteration 788, loss = 0.02025520\n",
            "Iteration 789, loss = 0.02016740\n",
            "Iteration 790, loss = 0.02009986\n",
            "Iteration 791, loss = 0.02003795\n",
            "Iteration 792, loss = 0.01997867\n",
            "Iteration 793, loss = 0.01993470\n",
            "Iteration 794, loss = 0.01985818\n",
            "Iteration 795, loss = 0.01976796\n",
            "Iteration 796, loss = 0.01977527\n",
            "Iteration 797, loss = 0.01967166\n",
            "Iteration 798, loss = 0.01963167\n",
            "Iteration 799, loss = 0.01954511\n",
            "Iteration 800, loss = 0.01948306\n",
            "Iteration 801, loss = 0.01943038\n",
            "Iteration 802, loss = 0.01933717\n",
            "Iteration 803, loss = 0.01929632\n",
            "Iteration 804, loss = 0.01926532\n",
            "Iteration 805, loss = 0.01917837\n",
            "Iteration 806, loss = 0.01912357\n",
            "Iteration 807, loss = 0.01906998\n",
            "Iteration 808, loss = 0.01901018\n",
            "Iteration 809, loss = 0.01895486\n",
            "Iteration 810, loss = 0.01888635\n",
            "Iteration 811, loss = 0.01879474\n",
            "Iteration 812, loss = 0.01873296\n",
            "Iteration 813, loss = 0.01867614\n",
            "Iteration 814, loss = 0.01861636\n",
            "Iteration 815, loss = 0.01856848\n",
            "Iteration 816, loss = 0.01850179\n",
            "Iteration 817, loss = 0.01845988\n",
            "Iteration 818, loss = 0.01838631\n",
            "Iteration 819, loss = 0.01833494\n",
            "Iteration 820, loss = 0.01829829\n",
            "Iteration 821, loss = 0.01821561\n",
            "Iteration 822, loss = 0.01816246\n",
            "Iteration 823, loss = 0.01809018\n",
            "Iteration 824, loss = 0.01803592\n",
            "Iteration 825, loss = 0.01797683\n",
            "Iteration 826, loss = 0.01793493\n",
            "Iteration 827, loss = 0.01785892\n",
            "Iteration 828, loss = 0.01784327\n",
            "Iteration 829, loss = 0.01773261\n",
            "Iteration 830, loss = 0.01770739\n",
            "Iteration 831, loss = 0.01764692\n",
            "Iteration 832, loss = 0.01758329\n",
            "Iteration 833, loss = 0.01753785\n",
            "Iteration 834, loss = 0.01747291\n",
            "Iteration 835, loss = 0.01741692\n",
            "Iteration 836, loss = 0.01735812\n",
            "Iteration 837, loss = 0.01730007\n",
            "Iteration 838, loss = 0.01723886\n",
            "Iteration 839, loss = 0.01718192\n",
            "Iteration 840, loss = 0.01712670\n",
            "Iteration 841, loss = 0.01708424\n",
            "Iteration 842, loss = 0.01704426\n",
            "Iteration 843, loss = 0.01703460\n",
            "Iteration 844, loss = 0.01692771\n",
            "Iteration 845, loss = 0.01688519\n",
            "Iteration 846, loss = 0.01680717\n",
            "Iteration 847, loss = 0.01679875\n",
            "Iteration 848, loss = 0.01671640\n",
            "Iteration 849, loss = 0.01667828\n",
            "Iteration 850, loss = 0.01665113\n",
            "Iteration 851, loss = 0.01653412\n",
            "Iteration 852, loss = 0.01652013\n",
            "Iteration 853, loss = 0.01645450\n",
            "Iteration 854, loss = 0.01642577\n",
            "Iteration 855, loss = 0.01635950\n",
            "Iteration 856, loss = 0.01630317\n",
            "Iteration 857, loss = 0.01624981\n",
            "Iteration 858, loss = 0.01619861\n",
            "Iteration 859, loss = 0.01613289\n",
            "Iteration 860, loss = 0.01611659\n",
            "Iteration 861, loss = 0.01603002\n",
            "Iteration 862, loss = 0.01600353\n",
            "Iteration 863, loss = 0.01594086\n",
            "Iteration 864, loss = 0.01589049\n",
            "Iteration 865, loss = 0.01585775\n",
            "Iteration 866, loss = 0.01578808\n",
            "Iteration 867, loss = 0.01575458\n",
            "Iteration 868, loss = 0.01567328\n",
            "Iteration 869, loss = 0.01567562\n",
            "Iteration 870, loss = 0.01559210\n",
            "Iteration 871, loss = 0.01555695\n",
            "Iteration 872, loss = 0.01549786\n",
            "Iteration 873, loss = 0.01545026\n",
            "Iteration 874, loss = 0.01540010\n",
            "Iteration 875, loss = 0.01534534\n",
            "Iteration 876, loss = 0.01528593\n",
            "Iteration 877, loss = 0.01523783\n",
            "Iteration 878, loss = 0.01519588\n",
            "Iteration 879, loss = 0.01512938\n",
            "Iteration 880, loss = 0.01513686\n",
            "Iteration 881, loss = 0.01505085\n",
            "Iteration 882, loss = 0.01498036\n",
            "Iteration 883, loss = 0.01491508\n",
            "Iteration 884, loss = 0.01488109\n",
            "Iteration 885, loss = 0.01483882\n",
            "Iteration 886, loss = 0.01477710\n",
            "Iteration 887, loss = 0.01471920\n",
            "Iteration 888, loss = 0.01469161\n",
            "Iteration 889, loss = 0.01465185\n",
            "Iteration 890, loss = 0.01457874\n",
            "Iteration 891, loss = 0.01453505\n",
            "Iteration 892, loss = 0.01448716\n",
            "Iteration 893, loss = 0.01442148\n",
            "Iteration 894, loss = 0.01438199\n",
            "Iteration 895, loss = 0.01434564\n",
            "Iteration 896, loss = 0.01428333\n",
            "Iteration 897, loss = 0.01422521\n",
            "Iteration 898, loss = 0.01421723\n",
            "Iteration 899, loss = 0.01413398\n",
            "Iteration 900, loss = 0.01409656\n",
            "Iteration 901, loss = 0.01404748\n",
            "Iteration 902, loss = 0.01399642\n",
            "Iteration 903, loss = 0.01394901\n",
            "Iteration 904, loss = 0.01390475\n",
            "Iteration 905, loss = 0.01385858\n",
            "Iteration 906, loss = 0.01380648\n",
            "Iteration 907, loss = 0.01375329\n",
            "Iteration 908, loss = 0.01370655\n",
            "Iteration 909, loss = 0.01368088\n",
            "Iteration 910, loss = 0.01361562\n",
            "Iteration 911, loss = 0.01358429\n",
            "Iteration 912, loss = 0.01354837\n",
            "Iteration 913, loss = 0.01348361\n",
            "Iteration 914, loss = 0.01343058\n",
            "Iteration 915, loss = 0.01339680\n",
            "Iteration 916, loss = 0.01334439\n",
            "Iteration 917, loss = 0.01332264\n",
            "Iteration 918, loss = 0.01325342\n",
            "Iteration 919, loss = 0.01323130\n",
            "Iteration 920, loss = 0.01316663\n",
            "Iteration 921, loss = 0.01314094\n",
            "Iteration 922, loss = 0.01310867\n",
            "Iteration 923, loss = 0.01307122\n",
            "Iteration 924, loss = 0.01299342\n",
            "Iteration 925, loss = 0.01297176\n",
            "Iteration 926, loss = 0.01292089\n",
            "Iteration 927, loss = 0.01287532\n",
            "Iteration 928, loss = 0.01286087\n",
            "Iteration 929, loss = 0.01277050\n",
            "Iteration 930, loss = 0.01273799\n",
            "Iteration 931, loss = 0.01268732\n",
            "Iteration 932, loss = 0.01263941\n",
            "Iteration 933, loss = 0.01263591\n",
            "Iteration 934, loss = 0.01257344\n",
            "Iteration 935, loss = 0.01252863\n",
            "Iteration 936, loss = 0.01248486\n",
            "Iteration 937, loss = 0.01245864\n",
            "Iteration 938, loss = 0.01239715\n",
            "Iteration 939, loss = 0.01237305\n",
            "Iteration 940, loss = 0.01234006\n",
            "Iteration 941, loss = 0.01228988\n",
            "Iteration 942, loss = 0.01224369\n",
            "Iteration 943, loss = 0.01219207\n",
            "Iteration 944, loss = 0.01215391\n",
            "Iteration 945, loss = 0.01212012\n",
            "Iteration 946, loss = 0.01207749\n",
            "Iteration 947, loss = 0.01205979\n",
            "Iteration 948, loss = 0.01202611\n",
            "Iteration 949, loss = 0.01195347\n",
            "Iteration 950, loss = 0.01191426\n",
            "Iteration 951, loss = 0.01187181\n",
            "Iteration 952, loss = 0.01182736\n",
            "Iteration 953, loss = 0.01180907\n",
            "Iteration 954, loss = 0.01176914\n",
            "Iteration 955, loss = 0.01171317\n",
            "Iteration 956, loss = 0.01167867\n",
            "Iteration 957, loss = 0.01167536\n",
            "Iteration 958, loss = 0.01161230\n",
            "Iteration 959, loss = 0.01159546\n",
            "Iteration 960, loss = 0.01151352\n",
            "Iteration 961, loss = 0.01150685\n",
            "Iteration 962, loss = 0.01149788\n",
            "Iteration 963, loss = 0.01140734\n",
            "Iteration 964, loss = 0.01140876\n",
            "Iteration 965, loss = 0.01133581\n",
            "Iteration 966, loss = 0.01133396\n",
            "Iteration 967, loss = 0.01128244\n",
            "Iteration 968, loss = 0.01127532\n",
            "Iteration 969, loss = 0.01122823\n",
            "Iteration 970, loss = 0.01115303\n",
            "Iteration 971, loss = 0.01112681\n",
            "Iteration 972, loss = 0.01110116\n",
            "Iteration 973, loss = 0.01108716\n",
            "Iteration 974, loss = 0.01101899\n",
            "Iteration 975, loss = 0.01097207\n",
            "Iteration 976, loss = 0.01099540\n",
            "Iteration 977, loss = 0.01089583\n",
            "Iteration 978, loss = 0.01088767\n",
            "Iteration 979, loss = 0.01084043\n",
            "Iteration 980, loss = 0.01080980\n",
            "Iteration 981, loss = 0.01077869\n",
            "Iteration 982, loss = 0.01073857\n",
            "Iteration 983, loss = 0.01071446\n",
            "Iteration 984, loss = 0.01067795\n",
            "Iteration 985, loss = 0.01065978\n",
            "Iteration 986, loss = 0.01059754\n",
            "Iteration 987, loss = 0.01057736\n",
            "Iteration 988, loss = 0.01058364\n",
            "Iteration 989, loss = 0.01050363\n",
            "Iteration 990, loss = 0.01048083\n",
            "Iteration 991, loss = 0.01046639\n",
            "Iteration 992, loss = 0.01044808\n",
            "Iteration 993, loss = 0.01038839\n",
            "Iteration 994, loss = 0.01035677\n",
            "Iteration 995, loss = 0.01033767\n",
            "Iteration 996, loss = 0.01030165\n",
            "Iteration 997, loss = 0.01025289\n",
            "Iteration 998, loss = 0.01025410\n",
            "Iteration 999, loss = 0.01018758\n",
            "Iteration 1000, loss = 0.01020296\n",
            "Iteration 1001, loss = 0.01014516\n",
            "Iteration 1002, loss = 0.01011074\n",
            "Iteration 1003, loss = 0.01007640\n",
            "Iteration 1004, loss = 0.01002593\n",
            "Iteration 1005, loss = 0.01000825\n",
            "Iteration 1006, loss = 0.00997434\n",
            "Iteration 1007, loss = 0.00995794\n",
            "Iteration 1008, loss = 0.00992414\n",
            "Iteration 1009, loss = 0.00990990\n",
            "Iteration 1010, loss = 0.00985070\n",
            "Iteration 1011, loss = 0.00983089\n",
            "Iteration 1012, loss = 0.00979485\n",
            "Iteration 1013, loss = 0.00977696\n",
            "Iteration 1014, loss = 0.00973592\n",
            "Iteration 1015, loss = 0.00970491\n",
            "Iteration 1016, loss = 0.00968989\n",
            "Iteration 1017, loss = 0.00963930\n",
            "Iteration 1018, loss = 0.00961049\n",
            "Iteration 1019, loss = 0.00958766\n",
            "Iteration 1020, loss = 0.00956490\n",
            "Iteration 1021, loss = 0.00954125\n",
            "Iteration 1022, loss = 0.00950269\n",
            "Iteration 1023, loss = 0.00949170\n",
            "Iteration 1024, loss = 0.00943984\n",
            "Iteration 1025, loss = 0.00942759\n",
            "Iteration 1026, loss = 0.00940402\n",
            "Iteration 1027, loss = 0.00935471\n",
            "Iteration 1028, loss = 0.00935519\n",
            "Iteration 1029, loss = 0.00931256\n",
            "Iteration 1030, loss = 0.00927609\n",
            "Iteration 1031, loss = 0.00925975\n",
            "Iteration 1032, loss = 0.00921162\n",
            "Iteration 1033, loss = 0.00919042\n",
            "Iteration 1034, loss = 0.00918575\n",
            "Iteration 1035, loss = 0.00913594\n",
            "Iteration 1036, loss = 0.00909488\n",
            "Iteration 1037, loss = 0.00908841\n",
            "Iteration 1038, loss = 0.00911109\n",
            "Iteration 1039, loss = 0.00902913\n",
            "Iteration 1040, loss = 0.00899740\n",
            "Iteration 1041, loss = 0.00896156\n",
            "Iteration 1042, loss = 0.00895084\n",
            "Iteration 1043, loss = 0.00891879\n",
            "Iteration 1044, loss = 0.00891000\n",
            "Iteration 1045, loss = 0.00886669\n",
            "Iteration 1046, loss = 0.00882932\n",
            "Iteration 1047, loss = 0.00881237\n",
            "Iteration 1048, loss = 0.00877579\n",
            "Iteration 1049, loss = 0.00877677\n",
            "Iteration 1050, loss = 0.00875436\n",
            "Iteration 1051, loss = 0.00871243\n",
            "Iteration 1052, loss = 0.00867832\n",
            "Iteration 1053, loss = 0.00865450\n",
            "Iteration 1054, loss = 0.00865793\n",
            "Iteration 1055, loss = 0.00860321\n",
            "Iteration 1056, loss = 0.00860012\n",
            "Iteration 1057, loss = 0.00855771\n",
            "Iteration 1058, loss = 0.00852193\n",
            "Iteration 1059, loss = 0.00850511\n",
            "Iteration 1060, loss = 0.00849016\n",
            "Iteration 1061, loss = 0.00844908\n",
            "Iteration 1062, loss = 0.00844611\n",
            "Iteration 1063, loss = 0.00841281\n",
            "Iteration 1064, loss = 0.00839684\n",
            "Iteration 1065, loss = 0.00838857\n",
            "Iteration 1066, loss = 0.00834337\n",
            "Iteration 1067, loss = 0.00829543\n",
            "Iteration 1068, loss = 0.00828116\n",
            "Iteration 1069, loss = 0.00827345\n",
            "Iteration 1070, loss = 0.00823850\n",
            "Iteration 1071, loss = 0.00820247\n",
            "Iteration 1072, loss = 0.00818213\n",
            "Iteration 1073, loss = 0.00815741\n",
            "Iteration 1074, loss = 0.00813299\n",
            "Iteration 1075, loss = 0.00810920\n",
            "Iteration 1076, loss = 0.00809700\n",
            "Iteration 1077, loss = 0.00807156\n",
            "Iteration 1078, loss = 0.00804263\n",
            "Iteration 1079, loss = 0.00801195\n",
            "Iteration 1080, loss = 0.00798971\n",
            "Iteration 1081, loss = 0.00796378\n",
            "Iteration 1082, loss = 0.00795239\n",
            "Iteration 1083, loss = 0.00793342\n",
            "Iteration 1084, loss = 0.00789747\n",
            "Iteration 1085, loss = 0.00787509\n",
            "Iteration 1086, loss = 0.00786375\n",
            "Iteration 1087, loss = 0.00786031\n",
            "Iteration 1088, loss = 0.00780918\n",
            "Iteration 1089, loss = 0.00777963\n",
            "Iteration 1090, loss = 0.00777883\n",
            "Iteration 1091, loss = 0.00775582\n",
            "Iteration 1092, loss = 0.00772243\n",
            "Iteration 1093, loss = 0.00768241\n",
            "Iteration 1094, loss = 0.00767894\n",
            "Iteration 1095, loss = 0.00763045\n",
            "Iteration 1096, loss = 0.00762616\n",
            "Iteration 1097, loss = 0.00761661\n",
            "Iteration 1098, loss = 0.00758346\n",
            "Iteration 1099, loss = 0.00755884\n",
            "Iteration 1100, loss = 0.00755261\n",
            "Iteration 1101, loss = 0.00751439\n",
            "Iteration 1102, loss = 0.00752196\n",
            "Iteration 1103, loss = 0.00747526\n",
            "Iteration 1104, loss = 0.00745524\n",
            "Iteration 1105, loss = 0.00746052\n",
            "Iteration 1106, loss = 0.00738980\n",
            "Iteration 1107, loss = 0.00742997\n",
            "Iteration 1108, loss = 0.00739009\n",
            "Iteration 1109, loss = 0.00741113\n",
            "Iteration 1110, loss = 0.00733095\n",
            "Iteration 1111, loss = 0.00729645\n",
            "Iteration 1112, loss = 0.00729827\n",
            "Iteration 1113, loss = 0.00726442\n",
            "Iteration 1114, loss = 0.00725898\n",
            "Iteration 1115, loss = 0.00722556\n",
            "Iteration 1116, loss = 0.00720361\n",
            "Iteration 1117, loss = 0.00718890\n",
            "Iteration 1118, loss = 0.00717027\n",
            "Iteration 1119, loss = 0.00714681\n",
            "Iteration 1120, loss = 0.00712457\n",
            "Iteration 1121, loss = 0.00712074\n",
            "Iteration 1122, loss = 0.00709920\n",
            "Iteration 1123, loss = 0.00710615\n",
            "Iteration 1124, loss = 0.00705637\n",
            "Iteration 1125, loss = 0.00702404\n",
            "Iteration 1126, loss = 0.00701132\n",
            "Iteration 1127, loss = 0.00697782\n",
            "Iteration 1128, loss = 0.00696259\n",
            "Iteration 1129, loss = 0.00695360\n",
            "Iteration 1130, loss = 0.00692588\n",
            "Iteration 1131, loss = 0.00691448\n",
            "Iteration 1132, loss = 0.00689401\n",
            "Iteration 1133, loss = 0.00686177\n",
            "Iteration 1134, loss = 0.00684008\n",
            "Iteration 1135, loss = 0.00681630\n",
            "Iteration 1136, loss = 0.00681542\n",
            "Iteration 1137, loss = 0.00680359\n",
            "Iteration 1138, loss = 0.00677559\n",
            "Iteration 1139, loss = 0.00675501\n",
            "Iteration 1140, loss = 0.00675039\n",
            "Iteration 1141, loss = 0.00674049\n",
            "Iteration 1142, loss = 0.00671353\n",
            "Iteration 1143, loss = 0.00669564\n",
            "Iteration 1144, loss = 0.00667469\n",
            "Iteration 1145, loss = 0.00667120\n",
            "Iteration 1146, loss = 0.00663434\n",
            "Iteration 1147, loss = 0.00660873\n",
            "Iteration 1148, loss = 0.00658476\n",
            "Iteration 1149, loss = 0.00657849\n",
            "Iteration 1150, loss = 0.00654720\n",
            "Iteration 1151, loss = 0.00654327\n",
            "Iteration 1152, loss = 0.00651676\n",
            "Iteration 1153, loss = 0.00650468\n",
            "Iteration 1154, loss = 0.00649206\n",
            "Iteration 1155, loss = 0.00648750\n",
            "Iteration 1156, loss = 0.00647920\n",
            "Iteration 1157, loss = 0.00642622\n",
            "Iteration 1158, loss = 0.00640869\n",
            "Iteration 1159, loss = 0.00638624\n",
            "Iteration 1160, loss = 0.00639071\n",
            "Iteration 1161, loss = 0.00635960\n",
            "Iteration 1162, loss = 0.00634133\n",
            "Iteration 1163, loss = 0.00632305\n",
            "Iteration 1164, loss = 0.00632677\n",
            "Iteration 1165, loss = 0.00628598\n",
            "Iteration 1166, loss = 0.00627004\n",
            "Iteration 1167, loss = 0.00626131\n",
            "Iteration 1168, loss = 0.00623797\n",
            "Iteration 1169, loss = 0.00623943\n",
            "Iteration 1170, loss = 0.00619998\n",
            "Iteration 1171, loss = 0.00620563\n",
            "Iteration 1172, loss = 0.00619545\n",
            "Iteration 1173, loss = 0.00614634\n",
            "Iteration 1174, loss = 0.00614136\n",
            "Iteration 1175, loss = 0.00612848\n",
            "Iteration 1176, loss = 0.00611619\n",
            "Iteration 1177, loss = 0.00608286\n",
            "Iteration 1178, loss = 0.00611416\n",
            "Iteration 1179, loss = 0.00607103\n",
            "Iteration 1180, loss = 0.00604751\n",
            "Iteration 1181, loss = 0.00601688\n",
            "Iteration 1182, loss = 0.00600377\n",
            "Iteration 1183, loss = 0.00599743\n",
            "Iteration 1184, loss = 0.00596688\n",
            "Iteration 1185, loss = 0.00598286\n",
            "Iteration 1186, loss = 0.00594865\n",
            "Iteration 1187, loss = 0.00592549\n",
            "Iteration 1188, loss = 0.00592222\n",
            "Iteration 1189, loss = 0.00589847\n",
            "Iteration 1190, loss = 0.00587410\n",
            "Iteration 1191, loss = 0.00588391\n",
            "Iteration 1192, loss = 0.00585161\n",
            "Iteration 1193, loss = 0.00586548\n",
            "Iteration 1194, loss = 0.00583799\n",
            "Iteration 1195, loss = 0.00584709\n",
            "Iteration 1196, loss = 0.00580904\n",
            "Iteration 1197, loss = 0.00577766\n",
            "Iteration 1198, loss = 0.00575992\n",
            "Iteration 1199, loss = 0.00576503\n",
            "Iteration 1200, loss = 0.00573053\n",
            "Iteration 1201, loss = 0.00570915\n",
            "Iteration 1202, loss = 0.00568757\n",
            "Iteration 1203, loss = 0.00568733\n",
            "Iteration 1204, loss = 0.00567917\n",
            "Iteration 1205, loss = 0.00565934\n",
            "Iteration 1206, loss = 0.00564385\n",
            "Iteration 1207, loss = 0.00563982\n",
            "Iteration 1208, loss = 0.00562582\n",
            "Iteration 1209, loss = 0.00560541\n",
            "Iteration 1210, loss = 0.00558122\n",
            "Iteration 1211, loss = 0.00559067\n",
            "Iteration 1212, loss = 0.00555748\n",
            "Iteration 1213, loss = 0.00555650\n",
            "Iteration 1214, loss = 0.00552927\n",
            "Iteration 1215, loss = 0.00553527\n",
            "Iteration 1216, loss = 0.00549294\n",
            "Iteration 1217, loss = 0.00550524\n",
            "Iteration 1218, loss = 0.00547624\n",
            "Iteration 1219, loss = 0.00545499\n",
            "Iteration 1220, loss = 0.00544026\n",
            "Iteration 1221, loss = 0.00544419\n",
            "Iteration 1222, loss = 0.00542028\n",
            "Iteration 1223, loss = 0.00539638\n",
            "Iteration 1224, loss = 0.00538147\n",
            "Iteration 1225, loss = 0.00536290\n",
            "Iteration 1226, loss = 0.00535068\n",
            "Iteration 1227, loss = 0.00535318\n",
            "Iteration 1228, loss = 0.00533581\n",
            "Iteration 1229, loss = 0.00530305\n",
            "Iteration 1230, loss = 0.00530914\n",
            "Iteration 1231, loss = 0.00529294\n",
            "Iteration 1232, loss = 0.00526382\n",
            "Iteration 1233, loss = 0.00528784\n",
            "Iteration 1234, loss = 0.00524901\n",
            "Iteration 1235, loss = 0.00525168\n",
            "Iteration 1236, loss = 0.00524668\n",
            "Iteration 1237, loss = 0.00527461\n",
            "Iteration 1238, loss = 0.00519943\n",
            "Iteration 1239, loss = 0.00519809\n",
            "Iteration 1240, loss = 0.00518020\n",
            "Iteration 1241, loss = 0.00516920\n",
            "Iteration 1242, loss = 0.00515998\n",
            "Iteration 1243, loss = 0.00514337\n",
            "Iteration 1244, loss = 0.00513402\n",
            "Iteration 1245, loss = 0.00509896\n",
            "Iteration 1246, loss = 0.00510416\n",
            "Iteration 1247, loss = 0.00507685\n",
            "Iteration 1248, loss = 0.00506415\n",
            "Iteration 1249, loss = 0.00505622\n",
            "Iteration 1250, loss = 0.00505023\n",
            "Iteration 1251, loss = 0.00504729\n",
            "Iteration 1252, loss = 0.00503660\n",
            "Iteration 1253, loss = 0.00501052\n",
            "Iteration 1254, loss = 0.00499913\n",
            "Iteration 1255, loss = 0.00497980\n",
            "Iteration 1256, loss = 0.00498923\n",
            "Iteration 1257, loss = 0.00496137\n",
            "Iteration 1258, loss = 0.00499829\n",
            "Iteration 1259, loss = 0.00497578\n",
            "Iteration 1260, loss = 0.00492135\n",
            "Iteration 1261, loss = 0.00490646\n",
            "Iteration 1262, loss = 0.00495312\n",
            "Iteration 1263, loss = 0.00491605\n",
            "Iteration 1264, loss = 0.00496292\n",
            "Iteration 1265, loss = 0.00487186\n",
            "Iteration 1266, loss = 0.00486926\n",
            "Iteration 1267, loss = 0.00484727\n",
            "Iteration 1268, loss = 0.00484012\n",
            "Iteration 1269, loss = 0.00481686\n",
            "Iteration 1270, loss = 0.00480615\n",
            "Iteration 1271, loss = 0.00486619\n",
            "Iteration 1272, loss = 0.00482506\n",
            "Iteration 1273, loss = 0.00479705\n",
            "Iteration 1274, loss = 0.00474789\n",
            "Iteration 1275, loss = 0.00476979\n",
            "Iteration 1276, loss = 0.00479053\n",
            "Iteration 1277, loss = 0.00474412\n",
            "Iteration 1278, loss = 0.00470164\n",
            "Iteration 1279, loss = 0.00470982\n",
            "Iteration 1280, loss = 0.00468525\n",
            "Iteration 1281, loss = 0.00467869\n",
            "Iteration 1282, loss = 0.00465676\n",
            "Iteration 1283, loss = 0.00468200\n",
            "Iteration 1284, loss = 0.00464604\n",
            "Iteration 1285, loss = 0.00464565\n",
            "Iteration 1286, loss = 0.00463543\n",
            "Iteration 1287, loss = 0.00467318\n",
            "Iteration 1288, loss = 0.00461305\n",
            "Iteration 1289, loss = 0.00463141\n",
            "Iteration 1290, loss = 0.00457537\n",
            "Iteration 1291, loss = 0.00457249\n",
            "Iteration 1292, loss = 0.00455699\n",
            "Iteration 1293, loss = 0.00458069\n",
            "Iteration 1294, loss = 0.00456854\n",
            "Iteration 1295, loss = 0.00453354\n",
            "Iteration 1296, loss = 0.00454111\n",
            "Iteration 1297, loss = 0.00452359\n",
            "Iteration 1298, loss = 0.00452743\n",
            "Iteration 1299, loss = 0.00451246\n",
            "Iteration 1300, loss = 0.00446848\n",
            "Iteration 1301, loss = 0.00446240\n",
            "Iteration 1302, loss = 0.00447094\n",
            "Iteration 1303, loss = 0.00445177\n",
            "Iteration 1304, loss = 0.00445009\n",
            "Iteration 1305, loss = 0.00444440\n",
            "Iteration 1306, loss = 0.00441430\n",
            "Iteration 1307, loss = 0.00440280\n",
            "Iteration 1308, loss = 0.00439347\n",
            "Iteration 1309, loss = 0.00438730\n",
            "Iteration 1310, loss = 0.00436169\n",
            "Iteration 1311, loss = 0.00435583\n",
            "Iteration 1312, loss = 0.00435765\n",
            "Iteration 1313, loss = 0.00435505\n",
            "Iteration 1314, loss = 0.00432611\n",
            "Iteration 1315, loss = 0.00433821\n",
            "Iteration 1316, loss = 0.00433807\n",
            "Iteration 1317, loss = 0.00432267\n",
            "Iteration 1318, loss = 0.00430271\n",
            "Iteration 1319, loss = 0.00435107\n",
            "Iteration 1320, loss = 0.00430733\n",
            "Iteration 1321, loss = 0.00427652\n",
            "Iteration 1322, loss = 0.00424776\n",
            "Iteration 1323, loss = 0.00423876\n",
            "Iteration 1324, loss = 0.00426622\n",
            "Iteration 1325, loss = 0.00422145\n",
            "Iteration 1326, loss = 0.00423749\n",
            "Iteration 1327, loss = 0.00421858\n",
            "Iteration 1328, loss = 0.00419662\n",
            "Iteration 1329, loss = 0.00418639\n",
            "Iteration 1330, loss = 0.00422078\n",
            "Iteration 1331, loss = 0.00420416\n",
            "Iteration 1332, loss = 0.00419179\n",
            "Iteration 1333, loss = 0.00415251\n",
            "Iteration 1334, loss = 0.00414132\n",
            "Iteration 1335, loss = 0.00414459\n",
            "Iteration 1336, loss = 0.00412370\n",
            "Iteration 1337, loss = 0.00410500\n",
            "Iteration 1338, loss = 0.00412517\n",
            "Iteration 1339, loss = 0.00409990\n",
            "Iteration 1340, loss = 0.00410968\n",
            "Iteration 1341, loss = 0.00408926\n",
            "Iteration 1342, loss = 0.00405659\n",
            "Iteration 1343, loss = 0.00408423\n",
            "Iteration 1344, loss = 0.00407557\n",
            "Iteration 1345, loss = 0.00405190\n",
            "Iteration 1346, loss = 0.00404018\n",
            "Iteration 1347, loss = 0.00401476\n",
            "Iteration 1348, loss = 0.00400874\n",
            "Iteration 1349, loss = 0.00401158\n",
            "Iteration 1350, loss = 0.00399714\n",
            "Iteration 1351, loss = 0.00399199\n",
            "Iteration 1352, loss = 0.00402491\n",
            "Iteration 1353, loss = 0.00397903\n",
            "Iteration 1354, loss = 0.00395292\n",
            "Iteration 1355, loss = 0.00398787\n",
            "Iteration 1356, loss = 0.00397670\n",
            "Iteration 1357, loss = 0.00395478\n",
            "Iteration 1358, loss = 0.00396646\n",
            "Iteration 1359, loss = 0.00392273\n",
            "Iteration 1360, loss = 0.00392297\n",
            "Iteration 1361, loss = 0.00389629\n",
            "Iteration 1362, loss = 0.00389070\n",
            "Iteration 1363, loss = 0.00388547\n",
            "Iteration 1364, loss = 0.00387569\n",
            "Iteration 1365, loss = 0.00387039\n",
            "Iteration 1366, loss = 0.00389657\n",
            "Iteration 1367, loss = 0.00388046\n",
            "Iteration 1368, loss = 0.00385595\n",
            "Iteration 1369, loss = 0.00383273\n",
            "Iteration 1370, loss = 0.00384482\n",
            "Iteration 1371, loss = 0.00383311\n",
            "Iteration 1372, loss = 0.00381396\n",
            "Iteration 1373, loss = 0.00380475\n",
            "Iteration 1374, loss = 0.00378741\n",
            "Iteration 1375, loss = 0.00378288\n",
            "Iteration 1376, loss = 0.00380438\n",
            "Iteration 1377, loss = 0.00378374\n",
            "Iteration 1378, loss = 0.00377073\n",
            "Iteration 1379, loss = 0.00375686\n",
            "Iteration 1380, loss = 0.00374309\n",
            "Iteration 1381, loss = 0.00374469\n",
            "Iteration 1382, loss = 0.00374426\n",
            "Iteration 1383, loss = 0.00371296\n",
            "Iteration 1384, loss = 0.00373607\n",
            "Iteration 1385, loss = 0.00371873\n",
            "Iteration 1386, loss = 0.00371455\n",
            "Iteration 1387, loss = 0.00369925\n",
            "Iteration 1388, loss = 0.00368600\n",
            "Iteration 1389, loss = 0.00369493\n",
            "Iteration 1390, loss = 0.00372028\n",
            "Iteration 1391, loss = 0.00366649\n",
            "Iteration 1392, loss = 0.00365734\n",
            "Iteration 1393, loss = 0.00367103\n",
            "Iteration 1394, loss = 0.00364358\n",
            "Iteration 1395, loss = 0.00363130\n",
            "Iteration 1396, loss = 0.00364532\n",
            "Iteration 1397, loss = 0.00362487\n",
            "Iteration 1398, loss = 0.00362382\n",
            "Iteration 1399, loss = 0.00360726\n",
            "Iteration 1400, loss = 0.00359599\n",
            "Iteration 1401, loss = 0.00357452\n",
            "Iteration 1402, loss = 0.00364651\n",
            "Iteration 1403, loss = 0.00357866\n",
            "Iteration 1404, loss = 0.00357316\n",
            "Iteration 1405, loss = 0.00357330\n",
            "Iteration 1406, loss = 0.00356218\n",
            "Iteration 1407, loss = 0.00354563\n",
            "Iteration 1408, loss = 0.00356279\n",
            "Iteration 1409, loss = 0.00353605\n",
            "Iteration 1410, loss = 0.00352555\n",
            "Iteration 1411, loss = 0.00351740\n",
            "Iteration 1412, loss = 0.00352598\n",
            "Iteration 1413, loss = 0.00352645\n",
            "Iteration 1414, loss = 0.00349068\n",
            "Iteration 1415, loss = 0.00349495\n",
            "Iteration 1416, loss = 0.00348831\n",
            "Iteration 1417, loss = 0.00351436\n",
            "Iteration 1418, loss = 0.00345166\n",
            "Iteration 1419, loss = 0.00346934\n",
            "Iteration 1420, loss = 0.00346190\n",
            "Iteration 1421, loss = 0.00347843\n",
            "Iteration 1422, loss = 0.00345480\n",
            "Iteration 1423, loss = 0.00347501\n",
            "Iteration 1424, loss = 0.00342803\n",
            "Iteration 1425, loss = 0.00343952\n",
            "Iteration 1426, loss = 0.00345936\n",
            "Iteration 1427, loss = 0.00341890\n",
            "Iteration 1428, loss = 0.00341224\n",
            "Iteration 1429, loss = 0.00339075\n",
            "Iteration 1430, loss = 0.00338400\n",
            "Iteration 1431, loss = 0.00338962\n",
            "Iteration 1432, loss = 0.00337276\n",
            "Iteration 1433, loss = 0.00336795\n",
            "Iteration 1434, loss = 0.00336169\n",
            "Iteration 1435, loss = 0.00335913\n",
            "Iteration 1436, loss = 0.00334227\n",
            "Iteration 1437, loss = 0.00333830\n",
            "Iteration 1438, loss = 0.00333825\n",
            "Iteration 1439, loss = 0.00333853\n",
            "Iteration 1440, loss = 0.00332292\n",
            "Iteration 1441, loss = 0.00331957\n",
            "Iteration 1442, loss = 0.00330165\n",
            "Iteration 1443, loss = 0.00331135\n",
            "Iteration 1444, loss = 0.00332142\n",
            "Iteration 1445, loss = 0.00328456\n",
            "Iteration 1446, loss = 0.00328336\n",
            "Iteration 1447, loss = 0.00326834\n",
            "Iteration 1448, loss = 0.00327174\n",
            "Iteration 1449, loss = 0.00326941\n",
            "Iteration 1450, loss = 0.00326250\n",
            "Iteration 1451, loss = 0.00328820\n",
            "Iteration 1452, loss = 0.00326009\n",
            "Iteration 1453, loss = 0.00330206\n",
            "Iteration 1454, loss = 0.00322977\n",
            "Iteration 1455, loss = 0.00322325\n",
            "Iteration 1456, loss = 0.00323280\n",
            "Iteration 1457, loss = 0.00323072\n",
            "Iteration 1458, loss = 0.00322546\n",
            "Iteration 1459, loss = 0.00319522\n",
            "Iteration 1460, loss = 0.00320493\n",
            "Iteration 1461, loss = 0.00319711\n",
            "Iteration 1462, loss = 0.00319631\n",
            "Iteration 1463, loss = 0.00318947\n",
            "Iteration 1464, loss = 0.00317033\n",
            "Iteration 1465, loss = 0.00318010\n",
            "Iteration 1466, loss = 0.00318136\n",
            "Iteration 1467, loss = 0.00317317\n",
            "Iteration 1468, loss = 0.00314258\n",
            "Iteration 1469, loss = 0.00315703\n",
            "Iteration 1470, loss = 0.00312246\n",
            "Iteration 1471, loss = 0.00313192\n",
            "Iteration 1472, loss = 0.00312412\n",
            "Iteration 1473, loss = 0.00311166\n",
            "Iteration 1474, loss = 0.00311594\n",
            "Iteration 1475, loss = 0.00309536\n",
            "Iteration 1476, loss = 0.00311585\n",
            "Iteration 1477, loss = 0.00311212\n",
            "Iteration 1478, loss = 0.00312295\n",
            "Iteration 1479, loss = 0.00312889\n",
            "Iteration 1480, loss = 0.00310912\n",
            "Iteration 1481, loss = 0.00308246\n",
            "Iteration 1482, loss = 0.00305700\n",
            "Iteration 1483, loss = 0.00306421\n",
            "Iteration 1484, loss = 0.00306099\n",
            "Iteration 1485, loss = 0.00305909\n",
            "Iteration 1486, loss = 0.00304771\n",
            "Iteration 1487, loss = 0.00306107\n",
            "Iteration 1488, loss = 0.00305833\n",
            "Iteration 1489, loss = 0.00304590\n",
            "Iteration 1490, loss = 0.00301641\n",
            "Iteration 1491, loss = 0.00301054\n",
            "Iteration 1492, loss = 0.00301185\n",
            "Iteration 1493, loss = 0.00301120\n",
            "Iteration 1494, loss = 0.00301354\n",
            "Iteration 1495, loss = 0.00301575\n",
            "Iteration 1496, loss = 0.00299580\n",
            "Iteration 1497, loss = 0.00297403\n",
            "Iteration 1498, loss = 0.00298476\n",
            "Iteration 1499, loss = 0.00298894\n",
            "Iteration 1500, loss = 0.00297206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes = redeneural_credit.predict(xtest)\n",
        "previsoes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtyE2zfA9BxS",
        "outputId": "a4c6af1c-3a0f-4064-dfaf-33bd3db82ae1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy_score(ytest, previsoes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6lTbSKqCAik",
        "outputId": "9f04499a-f09c-4f35-cc78-c7d08811c49e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9966666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cm = ConfusionMatrix(redeneural_credit)\n",
        "cm.fit(xtreino,ytreino)\n",
        "cm.score(xtest,ytest)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "cnZFOm5nCJTn",
        "outputId": "611140ca-66eb-4ae6-f594-fef6c3c2b9f7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9966666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(previsoes,ytest))"
      ],
      "metadata": {
        "id": "_hxb6_1hD4rj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e09d4c11-dec1-4174-bd03-5e7174d598df"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       259\n",
            "           1       1.00      0.98      0.99        41\n",
            "\n",
            "    accuracy                           1.00       300\n",
            "   macro avg       1.00      0.99      0.99       300\n",
            "weighted avg       1.00      1.00      1.00       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nRiGluwydKYy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}